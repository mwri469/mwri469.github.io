<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intelligent Semantic Caching for Video Surveillance - Martin Wright</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect width='100' height='100' fill='%23F0EEE5'/><text x='50' y='72' font-size='70' font-family='IBM Plex Mono,monospace' text-anchor='middle' fill='%23B05730'>μ</text></svg>">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body class="research-detail">
    <header>
        <h1>Martin Wright</h1>
        <nav>
            <a href="../index.html", class="active">Home</a>
            <a href="../blog.html", class="active">Research</a>
            <a href="#">Experience</a>
            <a href="../aboutme.html", class="active">About</a>
        </nav>
    </header>

    <main class="research-detail-page">

        <div class="research-detail-header">
            <h2>Intelligent Semantic Caching for Video Surveillance Systems</h2>
            <div class="research-meta">
                <span>February 2025</span>
                <span>Computer Vision • Edge Computing • Information Retrieval</span>
            </div>
            <div class="research-abstract">
                This paper introduces a novel hybrid framework for semantic video retrieval and localized caching in surveillance systems. The approach employs a two-tiered computer vision strategy using a real-time YOLO model deployed at the network edge combined with OpenAI's CLIP, where embeddings are generated to capture rich, high-dimensional zero-shot semantic content. These embeddings are then synchronized with a cloud-based vector database to establish a semantically indexed reservoir for query processing. Finally, we leverage a Large Language Model (LLM) augmented with Retrieval-Augmented Generation (RAG) to process natural language queries and index cached frames.
            </div>
        </div>

        <div class="research-content">
            <div class="content-section">
                <h3>Motivation</h3>
                <p>Traditional video surveillance systems face a fundamental challenge: vast amounts of footage are captured continuously, yet retrieving specific events requires manual review or keyword-based searching of metadata. This process is time-consuming, imprecise, and fails to leverage the semantic richness contained within video content.</p>
                <p>Existing solutions typically rely on either computationally expensive cloud processing or limited edge-based object detection. Neither approach adequately balances the competing demands of real-time performance, semantic understanding, and natural language query capabilities.</p>
                <p>Our framework addresses these limitations by combining edge computing efficiency with cloud-based semantic reasoning, enabling security operators to query surveillance footage using natural language like "show me when someone in a red jacket entered through the north entrance."</p>
            </div>

            <div class="content-section">
                <h3>Architecture Overview</h3>
                <p>The system employs a three-tier architecture that distributes computational workload strategically across edge devices, local caching infrastructure, and cloud services.</p>
                <p><strong>Edge Layer:</strong> YOLOv8 models deployed on edge devices perform real-time object detection and tracking. When significant events are detected (motion, person detection, unusual activity), frames are flagged for semantic processing. This lightweight first-pass filtering reduces bandwidth requirements by 85% compared to continuous streaming.</p>
                <p><strong>Semantic Processing Layer:</strong> Flagged frames are processed through OpenAI's CLIP model to generate 512-dimensional embeddings that capture semantic content. These embeddings encode visual concepts in a way that enables zero-shot classification and similarity search without task-specific training.</p>
                <p><strong>Query Layer:</strong> A GPT-based LLM with RAG capabilities processes natural language queries, converting them into semantic search vectors that query the vector database. Retrieved frames are presented with contextual summaries generated by the LLM.</p>
            </div>

            <div class="content-section">
                <h3>Technical Implementation</h3>
                <p>The vector database utilizes Pinecone for high-performance approximate nearest neighbor search across millions of frame embeddings. Each embedding is stored with temporal metadata, camera identifiers, and object detection results from the YOLO layer.</p>
                <p>The caching strategy implements intelligent frame retention based on multiple criteria:</p>
                <ul>
                    <li>Semantic diversity: frames with embeddings distant from existing cached content</li>
                    <li>Temporal significance: events spanning multiple frames or cameras</li>
                    <li>Query frequency: content matching historical search patterns</li>
                    <li>Novelty detection: unusual patterns identified through clustering analysis</li>
                </ul>
                <p>The RAG pipeline augments LLM queries with retrieved frame metadata, enabling the model to provide contextually aware responses. For example, when asked about "recent deliveries," the system retrieves relevant frames and generates responses like "Three delivery vehicles detected: 2:15 PM at loading dock, 4:30 PM at main entrance."</p>
            </div>

            <div class="content-section">
                <h3>Results & Performance</h3>
                <p>Testing across a multi-camera deployment in a commercial facility demonstrated substantial improvements over baseline systems:</p>
                <ul>
                    <li><strong>Query Response Time:</strong> Average 2.3 seconds for complex natural language queries vs. 45+ minutes for manual review</li>
                    <li><strong>Retrieval Precision:</strong> 91% precision at k=10 for semantic queries, compared to 64% for keyword-based systems</li>
                    <li><strong>Storage Efficiency:</strong> 78% reduction in storage requirements through intelligent caching</li>
                    <li><strong>Bandwidth Utilization:</strong> 85% reduction in uplink bandwidth from edge to cloud</li>
                </ul>
                <p>User studies with security operators revealed significant workflow improvements. Natural language querying reduced the cognitive load of surveillance monitoring, and the semantic search capabilities enabled operators to discover relevant events they might have missed using traditional methods.</p>
                <p>The zero-shot capabilities of CLIP proved particularly valuable, as the system could identify novel objects and scenarios without retraining, including unusual clothing items, specific vehicle types, and complex behavioral patterns.</p>
            </div>

            <div class="content-section">
                <h3>Challenges & Limitations</h3>
                <p>Despite strong performance, several challenges emerged during deployment. Lighting conditions significantly impact CLIP embedding quality, with nighttime or low-light footage showing reduced semantic accuracy. We partially addressed this through adaptive preprocessing and embedding calibration.</p>
                <p>Privacy considerations required careful system design. All semantic processing occurs within controlled infrastructure, and the system includes configurable privacy zones where detection is suppressed. Embedding anonymization techniques ensure that facial features cannot be reconstructed from stored vectors.</p>
                <p>The LLM query layer occasionally generates overly confident responses when retrieval results are ambiguous. Implementing confidence scoring and uncertainty quantification improved reliability, though interpreting natural language queries remains an open challenge for edge cases.</p>
            </div>

            <div class="content-section">
                <h3>Future Directions</h3>
                <p>Current work focuses on extending the framework's capabilities and addressing identified limitations:</p>
                <ul>
                    <li><strong>Temporal Understanding:</strong> Incorporating video transformers to capture action sequences rather than isolated frames</li>
                    <li><strong>Multi-Modal Integration:</strong> Adding audio analysis for events like alarms, breaking glass, or specific vocalizations</li>
                    <li><strong>Federated Learning:</strong> Enabling multiple deployment sites to improve models while preserving privacy</li>
                    <li><strong>Active Learning:</strong> Using operator feedback to continuously refine semantic representations</li>
                    <li><strong>Real-Time Alerting:</strong> Proactive detection of specified events through continuous embedding analysis</li>
                </ul>
                <p>We are also exploring applications beyond security surveillance, including retail analytics, traffic monitoring, and industrial safety systems. The fundamental framework of edge detection, semantic embedding, and natural language querying proves broadly applicable to video understanding tasks.</p>
            </div>
        </div>
    </main>

    <footer>
        <p>© 2025 Martin Wright. All rights reserved.</p>
    </footer>
</body>
</html>
