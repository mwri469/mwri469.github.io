<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Martin Wright - Research</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <h1>Martin Wright</h1>
        <nav>
            <a href="#" class="active">Home</a>
            <a href="#">Research</a>
            <a href="#">Publications</a>
            <a href="#">About</a>
        </nav>
    </header>

    <main>
        <div class="intro">
            <p>I am a researcher and engineer focused on the fields of machine learning, simulation and operations research. My work spans computer vision, data science, and their applications to real-world problems in scheduling, hydrology and engineering. </p>
        </div>

        <section class="research-section">
            <!-- <h2 class="research-heading">Recent Research</h2> -->

            <div class="research-item">
                <div class="research-title">Intelligent Semantic Caching for Video Surveillance Systems</div>
                <div class="research-date">February 2025</div>
                <div class="research-description">
                    This paper introduces a novel hybrid framework for semantic video retrieval and localized caching in surveillance systems, directly addressing the computational bottleneck of deep learning on high-volume video streams. Our approach employs a two-tiered computer vision strategy beginning with a lightweight, real-time YOLO model deployed at the network edge to efficiently filter and detect predefined scenes-of-interest. Upon detection, relevant video segments are extracted for intelligent caching, where OpenAI's CLIP embeddings are generated to capture rich, high-dimensional zero-shot semantic content. These embeddings are then synchronized with a cloud-based vector database to establish a semantically indexed reservoir for query processing. Finally, we leverage a Large Language Model (LLM) augmented with Retrieval-Augmented Generation (RAG), enabling users to submit descriptive natural language queries and achieve low-latency, high-precision retrieval of complex events that would be intractable using traditional keyword or timestamp methods.
                </div>
            </div>
        </section>
    </main>

    <footer>
        <p>Â© 2025 Martin Wright. All rights reserved.</p>
    </footer>
</body>
</html>
